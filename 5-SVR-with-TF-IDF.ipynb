{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8eec7a24",
   "metadata": {
    "id": "8eec7a24"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import sklearn\n",
    "import gensim # using v.8.3.1\n",
    "import nltk \n",
    "import logging\n",
    "import utils\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uJ3ujcJIJZj8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ3ujcJIJZj8",
    "outputId": "abec99a8-cfe4-4e4a-ae16-73b3a8fa693d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "LttLpUJ6rM6o",
   "metadata": {
    "id": "LttLpUJ6rM6o"
   },
   "outputs": [],
   "source": [
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words = list(set().union(nltk_stop_words, sklearn_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfe70cd",
   "metadata": {
    "id": "3dfe70cd"
   },
   "outputs": [],
   "source": [
    "with open(utils.DATA + 'tfidf_predicted_tags.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b74e4df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b74e4df",
    "outputId": "ddd613ec-96d7-4a2c-b400-f427a3536348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3502"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, target = [], []\n",
    "\n",
    "for doc in data:\n",
    "    if len(doc['normal_tags']) != 0:\n",
    "        corpus.append(doc['captions'])\n",
    "        target.append(doc['normal_tags'])\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sfV12EfdEizz",
   "metadata": {
    "id": "sfV12EfdEizz"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(corpus, target), columns=['doc', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oyIkt3onSJx0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyIkt3onSJx0",
    "outputId": "dd279978-9a33-4b9e-84c2-abbeb66014d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc\n",
      "tag\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9RLi3UvnATY8",
   "metadata": {
    "id": "9RLi3UvnATY8"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5vkZk6b2Gn4X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vkZk6b2Gn4X",
    "outputId": "63253d4d-2858-4aa2-f1dd-c6ff83e394d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bdb95b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bdb95b7",
    "outputId": "125143fd-d466-4a16-9dee-ea88e380fc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(utils.DATA + \"w2v_googlenews/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355e046",
   "metadata": {},
   "source": [
    "First of all, we need to tokenize our documents and get a word2vec vector representation for each word. After that we're going to find the mean of all word vectors in a document so as to create one vector that is going to represent the text. And, finally, we will get the most similiar value from word2vec's vocabulary and see if this naive representation makes any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0JTgWWVBvTu8",
   "metadata": {
    "id": "0JTgWWVBvTu8"
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_tags(tag_list):\n",
    "    sent = \"\"\n",
    "    for tag in tag_list:\n",
    "        sent = \" \".join([sent, tag])\n",
    "    tokens = w2v_tokenize_text(sent)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b779eb",
   "metadata": {
    "id": "d6b779eb"
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in sent_tokenize(text, language='english'):\n",
    "        for word in word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "DtFzx2lU9KzK",
   "metadata": {
    "id": "DtFzx2lU9KzK"
   },
   "outputs": [],
   "source": [
    "def w2v_transform_word(word):\n",
    "    try:\n",
    "        vector = wv.syn0norm[wv.vocab[word].index]\n",
    "    except Exception as exception:\n",
    "        # print(f\"{type(exception).__name__} occured for word '{word}'.\")\n",
    "        vector = np.zeros(wv.vector_size,)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "owGiGmgy-V6b",
   "metadata": {
    "id": "owGiGmgy-V6b"
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv, doc_list):\n",
    "    return np.vstack([word_averaging(wv, doc) for doc in doc_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bajztWig_Shs",
   "metadata": {
    "id": "bajztWig_Shs"
   },
   "outputs": [],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['doc']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['doc']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "i76eMWRGvGEe",
   "metadata": {
    "id": "i76eMWRGvGEe"
   },
   "outputs": [],
   "source": [
    "y_test_tokenized = test_data.apply(lambda r: w2v_tokenize_tags(r['tag']), axis=1).values\n",
    "y_train_tokenized = train_data.apply(lambda r: w2v_tokenize_tags(r['tag']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eovX0F9mJKl2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eovX0F9mJKl2",
    "outputId": "ad39c54d-fa12-47d8-8b7e-b2b3b4e07c4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexi\\AppData\\Local\\Temp/ipykernel_1952/1173782423.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n",
      "WARNING:root:cannot compute similarity with no input ['WEBVTT']\n",
      "WARNING:root:cannot compute similarity with no input ['WEBVTT']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_word_average = word_averaging_list(wv, train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv, test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "iGeMEM7LOSvp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGeMEM7LOSvp",
    "outputId": "9c5c3f94-d8a8-4d5c-dc8f-3cd836dc9902"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3151, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "AQUkFInPYSLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQUkFInPYSLO",
    "outputId": "a6fa8691-fec1-43b5-f640-5ed782bbf14a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexi\\AppData\\Local\\Temp/ipykernel_1952/1173782423.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n",
      "WARNING:root:cannot compute similarity with no input ['Prize\\\\nYouth']\n",
      "WARNING:root:cannot compute similarity with no input ['philoshophy']\n",
      "WARNING:root:cannot compute similarity with no input ['\\\\\\\\Jonny', 'Mizzone\\\\\\\\', '\\\\\\\\Tommy', 'Mizzone\\\\\\\\', '\\\\\\\\Robbie', 'Mizzone\\\\\\\\']\n",
      "WARNING:root:cannot compute similarity with no input ['Collaboration\\\\nGlobal', 'media\\\\nViolence\\\\nWar']\n",
      "WARNING:root:cannot compute similarity with no input ['Ananada']\n",
      "WARNING:root:cannot compute similarity with no input ['\\\\\\\\Disaster', 'relief\\\\\\\\', '\\\\\\\\global', 'issues\\\\\\\\']\n",
      "WARNING:root:cannot compute similarity with no input ['Brain\\\\ncognitive']\n",
      "WARNING:root:cannot compute similarity with no input ['2011G', '480p']\n"
     ]
    }
   ],
   "source": [
    "y_train_word_average = word_averaging_list(wv, y_train_tokenized)\n",
    "y_test_word_average = word_averaging_list(wv, y_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "z3XLsOZJz65u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3XLsOZJz65u",
    "outputId": "a8768dbc-b3c2-41dc-a720-dae0ae8ec6a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 0.6011865139007568),\n",
       " ('that', 0.582533597946167),\n",
       " ('actually', 0.5790467262268066),\n",
       " ('think', 0.5734384059906006),\n",
       " ('anyway', 0.5630047917366028),\n",
       " ('just', 0.5600370764732361),\n",
       " ('so', 0.5590255260467529),\n",
       " ('know', 0.551464319229126),\n",
       " ('but', 0.5503146052360535),\n",
       " ('guess', 0.5415164232254028)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=[X_test_word_average[100]], restrict_vocab=100000, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JR0kXFbdc7pG",
   "metadata": {
    "id": "JR0kXFbdc7pG"
   },
   "source": [
    "As we can see, the naive method isn't really helpful, since the text simlarity is close to words that are evidentally aren't important for understanding the topic. So now we're going to try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3f8dc",
   "metadata": {},
   "source": [
    "## Tag prediction based on TF-IDF \n",
    "\n",
    "Here we're going to make an assumption that words in each text that were considered the most important ones by the TF-IDF algorithm represent our documents well enough. So we will convert top-1 words (according to TF-IDF) into w2v vectors and use them to predict one tag that we already have in our dataframe as a target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9d818",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "oxIj8TYVJVVZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "oxIj8TYVJVVZ",
    "outputId": "c0a8f4e5-8943-4bcc-bb95-2f923493b33c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I was 15, I got my first professional aud...</td>\n",
       "      <td>[business, entertainment, social change, art, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[SHAPE YOUR FUTURE] It’s a warm morning and I’...</td>\n",
       "      <td>[storytelling, film, humanity, mental health, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Should I do a cleanse?\" I hear people asking ...</td>\n",
       "      <td>[food, health, health care, biology, human body]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SHAPE YOUR FUTURE] We are at the beginning of...</td>\n",
       "      <td>[culture, design, art, future, Moon, space, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whitney Pennington Rodgers: Each of us, no mat...</td>\n",
       "      <td>[Social Change, Society, Humanity, Activism, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "      <td>[education, educational, system, creativity, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>If you're here today -- and I'm very happy tha...</td>\n",
       "      <td>[Sustainable, South, Bronx, environment, green...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>With all the legitimate concerns about AIDS an...</td>\n",
       "      <td>[Conference, preventive, medicine, diabetes, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>This is really a two-hour presentation I give ...</td>\n",
       "      <td>[How to succeed, secrets for success, what lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>I wrote this poem after hearing a pretty well ...</td>\n",
       "      <td>[spoken word, def, poetry, jam, entertainment,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3502 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    doc  \\\n",
       "0     When I was 15, I got my first professional aud...   \n",
       "1     [SHAPE YOUR FUTURE] It’s a warm morning and I’...   \n",
       "2     \"Should I do a cleanse?\" I hear people asking ...   \n",
       "3     [SHAPE YOUR FUTURE] We are at the beginning of...   \n",
       "4     Whitney Pennington Rodgers: Each of us, no mat...   \n",
       "...                                                 ...   \n",
       "3497  Good morning. How are you? (Audience) Good. It...   \n",
       "3498  If you're here today -- and I'm very happy tha...   \n",
       "3499  With all the legitimate concerns about AIDS an...   \n",
       "3500  This is really a two-hour presentation I give ...   \n",
       "3501  I wrote this poem after hearing a pretty well ...   \n",
       "\n",
       "                                                    tag  \n",
       "0     [business, entertainment, social change, art, ...  \n",
       "1     [storytelling, film, humanity, mental health, ...  \n",
       "2      [food, health, health care, biology, human body]  \n",
       "3     [culture, design, art, future, Moon, space, an...  \n",
       "4     [Social Change, Society, Humanity, Activism, M...  \n",
       "...                                                 ...  \n",
       "3497  [education, educational, system, creativity, i...  \n",
       "3498  [Sustainable, South, Bronx, environment, green...  \n",
       "3499  [Conference, preventive, medicine, diabetes, d...  \n",
       "3500  [How to succeed, secrets for success, what lea...  \n",
       "3501  [spoken word, def, poetry, jam, entertainment,...  \n",
       "\n",
       "[3502 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GxGeRUYwKlC6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxGeRUYwKlC6",
    "outputId": "fc04d872-7f96-4129-b2a3-2e784fff73c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 3496\n"
     ]
    }
   ],
   "source": [
    "tf_idf_data = []\n",
    "\n",
    "for n in range(len(data)):\n",
    "    if (len(data[n]['normal_tags']) != 0) & (len(data[n]['tf_idf_predicted_tags']) >= 5):\n",
    "        row = {}\n",
    "        for i in range(5):\n",
    "            tokens = []\n",
    "            line = data[n]['tf_idf_predicted_tags'][i][0]\n",
    "            for word in word_tokenize(line, language='english'):\n",
    "                line = line.replace(' ', '_')\n",
    "            row[f'top_word_{i+1}'] = line\n",
    "        line = data[n]['normal_tags'][0]\n",
    "        row['target'] = line.replace(' ', '_')\n",
    "        tf_idf_data.append(row)\n",
    "\n",
    "print(f\"Total length: {len(tf_idf_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "XLBp_M42wCl9",
   "metadata": {
    "id": "XLBp_M42wCl9"
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tf_idf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "_HBusMwbLWoW",
   "metadata": {
    "id": "_HBusMwbLWoW"
   },
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df[['top_word_1', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "GNvE87aQLeKw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GNvE87aQLeKw",
    "outputId": "2c10b53c-23e9-4d8e-a43a-1b80fb0927ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_word_1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Latina</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adayanci</td>\n",
       "      <td>storytelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liver</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moon</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lAJ</td>\n",
       "      <td>Social_Change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top_word_1         target\n",
       "0     Latina       business\n",
       "1   Adayanci   storytelling\n",
       "2      liver           food\n",
       "3       Moon        culture\n",
       "4        lAJ  Social_Change"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bo1dEUJ0RqDF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo1dEUJ0RqDF",
    "outputId": "8dd12fee-d81c-4f41-861b-36570b4d13da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexi\\AppData\\Local\\Temp/ipykernel_1952/1610283242.py:3: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  vector = wv.syn0norm[wv.vocab[word].index]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "for col_name in tfidf_df.columns:\n",
    "    col = tfidf_df.apply(lambda r: w2v_transform_word(r[col_name]), axis=1).values\n",
    "    vectors.append(col)\n",
    "\n",
    "df_temp = pd.DataFrame({'top-word': vectors[0], 'target': vectors[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "A4jFbVL-y5wq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "A4jFbVL-y5wq",
    "outputId": "ad399a32-d6ac-45b4-8aa4-c61dc4938c4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top-word</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.05219498, -0.012211393, 0.08038586, 0.0415...</td>\n",
       "      <td>[0.0050679236, -0.023729809, -0.06200754, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.083809115, -0.054631125, -0.05711436, 0.040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.10085145, 0.06359244, -0.009875038, -0.020...</td>\n",
       "      <td>[-0.06801747, 0.061800815, -0.0621665, 0.13384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.040945165, 0.04616862, -0.031003747, -0.03...</td>\n",
       "      <td>[-0.0519085, 0.0729623, 0.05009352, 0.02504676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            top-word  \\\n",
       "0  [-0.05219498, -0.012211393, 0.08038586, 0.0415...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [-0.10085145, 0.06359244, -0.009875038, -0.020...   \n",
       "3  [-0.040945165, 0.04616862, -0.031003747, -0.03...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              target  \n",
       "0  [0.0050679236, -0.023729809, -0.06200754, -0.0...  \n",
       "1  [0.083809115, -0.054631125, -0.05711436, 0.040...  \n",
       "2  [-0.06801747, 0.061800815, -0.0621665, 0.13384...  \n",
       "3  [-0.0519085, 0.0729623, 0.05009352, 0.02504676...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "g5xiQFseMEpr",
   "metadata": {
    "id": "g5xiQFseMEpr"
   },
   "outputs": [],
   "source": [
    "data_flat = pd.DataFrame(df_temp['top-word'].to_list())\n",
    "y_flat = pd.DataFrame(df_temp['target'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "GwPnNa5iMLAm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "GwPnNa5iMLAm",
    "outputId": "58c82286-4a14-454b-a15b-3df288219865"
   },
   "outputs": [],
   "source": [
    "y_short = y_flat[:][0]\n",
    "data_flat['target'] = y_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31e517d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.052195</td>\n",
       "      <td>-0.012211</td>\n",
       "      <td>0.080386</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>-0.041309</td>\n",
       "      <td>-0.020655</td>\n",
       "      <td>0.061406</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>-0.080386</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.126161</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.036844</td>\n",
       "      <td>0.020934</td>\n",
       "      <td>0.005068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.100851</td>\n",
       "      <td>0.063592</td>\n",
       "      <td>-0.009875</td>\n",
       "      <td>-0.020310</td>\n",
       "      <td>-0.043702</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.072837</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>0.038380</td>\n",
       "      <td>0.049585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073397</td>\n",
       "      <td>-0.057149</td>\n",
       "      <td>-0.010575</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>-0.067234</td>\n",
       "      <td>-0.029695</td>\n",
       "      <td>0.100291</td>\n",
       "      <td>0.060231</td>\n",
       "      <td>-0.068017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040945</td>\n",
       "      <td>0.046169</td>\n",
       "      <td>-0.031004</td>\n",
       "      <td>-0.036227</td>\n",
       "      <td>0.052235</td>\n",
       "      <td>-0.038081</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.076161</td>\n",
       "      <td>-0.024769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048865</td>\n",
       "      <td>-0.034542</td>\n",
       "      <td>0.026117</td>\n",
       "      <td>0.026623</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>-0.005350</td>\n",
       "      <td>-0.076161</td>\n",
       "      <td>0.020136</td>\n",
       "      <td>-0.057964</td>\n",
       "      <td>-0.051909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.052195 -0.012211  0.080386  0.041589  0.033773 -0.041309 -0.020655   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2 -0.100851  0.063592 -0.009875 -0.020310 -0.043702  0.023532  0.072837   \n",
       "3 -0.040945  0.046169 -0.031004 -0.036227  0.052235 -0.038081  0.010868   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          7         8         9  ...       291       292       293       294  \\\n",
       "0  0.061406  0.045217 -0.020096  ...  0.025679 -0.080386  0.000567  0.020096   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2 -0.023392  0.038380  0.049585  ... -0.073397 -0.057149 -0.010575 -0.000994   \n",
       "3  0.013985  0.076161 -0.024769  ...  0.048865 -0.034542  0.026117  0.026623   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        295       296       297       298       299    target  \n",
       "0  0.082619  0.126161  0.025818  0.036844  0.020934  0.005068  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.083809  \n",
       "2  0.056029 -0.067234 -0.029695  0.100291  0.060231 -0.068017  \n",
       "3 -0.000405 -0.005350 -0.076161  0.020136 -0.057964 -0.051909  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e666b76",
   "metadata": {},
   "source": [
    "### Trying to predict one of the coordinates of the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cb85975",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_flat, test_size=0.1, random_state=42)\n",
    "\n",
    "y_train = train_df['target']\n",
    "X_train = train_df.drop(columns=['target'])\n",
    "y_test = test_df['target']\n",
    "X_test = test_df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "r7LkFAAx5IdM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "r7LkFAAx5IdM",
    "outputId": "3e60b78d-366a-444e-e30b-3ee329056be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train data:\t0.0013281210158799823\n",
      "MSE on test data:\t0.0020305063491727754\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svr = SVR(kernel='poly', degree=4, epsilon=0.05, C=3).fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svr.predict(X_train)\n",
    "y_pred_test = svr.predict(X_test)\n",
    "\n",
    "print(f\"MSE on train data:\\t{mse(y_train, y_pred_train)}\")\n",
    "print(f\"MSE on test data:\\t{mse(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "FgEGAqj9790y",
   "metadata": {
    "id": "FgEGAqj9790y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train data:\t0.001731760454196217\n",
      "MSE on test data:\t0.0021557951501767077\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linreg = linear_model.LinearRegression(n_jobs=1).fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = linreg.predict(X_train)\n",
    "y_pred_test = linreg.predict(X_test)\n",
    "\n",
    "print(f\"MSE on train data:\\t{mse(y_train, y_pred_train)}\")\n",
    "print(f\"MSE on test data:\\t{mse(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f3b2086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train data:\t0.0017709467750134233\n",
      "MSE on test data:\t0.0019875156822344825\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_depth=4, min_samples_split=5, min_samples_leaf=15, random_state=1)\\\n",
    "                            .fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rfr.predict(X_train)\n",
    "y_pred_test = rfr.predict(X_test)\n",
    "\n",
    "print(f\"MSE on train data:\\t{mse(y_train, y_pred_train)}\")\n",
    "print(f\"MSE on test data:\\t{mse(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cab29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'squared_error',\n",
    "          'random_state': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ee101c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train data:\t0.0013930917730346358\n",
      "MSE on test data:\t0.001974105617912839\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gbr = GradientBoostingRegressor(**params).fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = gbr.predict(X_train)\n",
    "y_pred_test = gbr.predict(X_test)\n",
    "\n",
    "print(f\"MSE on train data:\\t{mse(y_train, y_pred_train)}\")\n",
    "print(f\"MSE on test data:\\t{mse(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc377c",
   "metadata": {},
   "source": [
    "### Predicting tags using Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f452557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X_train, y_train, X_test, model):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "307e12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions done: 10\n",
      "Predictions done: 20\n",
      "Predictions done: 30\n",
      "Predictions done: 40\n",
      "Predictions done: 50\n",
      "Predictions done: 60\n",
      "Predictions done: 70\n",
      "Predictions done: 80\n",
      "Predictions done: 90\n",
      "Predictions done: 100\n",
      "Predictions done: 110\n",
      "Predictions done: 120\n",
      "Predictions done: 130\n",
      "Predictions done: 140\n",
      "Predictions done: 150\n",
      "Predictions done: 160\n",
      "Predictions done: 170\n",
      "Predictions done: 180\n",
      "Predictions done: 190\n",
      "Predictions done: 200\n",
      "Predictions done: 210\n",
      "Predictions done: 220\n",
      "Predictions done: 230\n",
      "Predictions done: 240\n",
      "Predictions done: 250\n",
      "Predictions done: 260\n",
      "Predictions done: 270\n",
      "Predictions done: 280\n",
      "Predictions done: 290\n",
      "Predictions done: 300\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prediction, mse_train, mse_test = [], [], []\n",
    "\n",
    "for i in range(len(y_flat.columns)):\n",
    "    data_flat[f'target_{i}'] = y_flat[:][i]\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(data_flat, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "for i in range(len(y_flat.columns)):\n",
    "    y_train = train_df[f'target_{i}']\n",
    "    X_train = train_df.iloc[:, :300]\n",
    "    y_test = test_df[f'target_{i}']\n",
    "    X_test = test_df.iloc[:, :300]\n",
    "    \n",
    "    svr = SVR(kernel='poly', degree=4, epsilon=0.05, C=3)\n",
    "    # linreg = linear_model.LinearRegression(n_jobs=1)\n",
    "    # gbr = GradientBoostingRegressor(**params).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = get_prediction(X_train, y_train, X_test, svr)\n",
    "\n",
    "    prediction.append(y_pred)\n",
    "    mse_train.append(mse(y_train, y_pred_train))\n",
    "    mse_test.append(mse(y_test, y_pred_test))\n",
    "    \n",
    "    if len(prediction) % 10 == 0:\n",
    "        print(f\"Predictions done: {len(prediction)}\")\n",
    "\n",
    "\n",
    "y_full = pd.DataFrame(zip(*prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed1aea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE on train data:\t0.0031595146821644044\n",
      "Mean MSE on test data:\t0.0031165329031722167\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean MSE on train data:\\t{np.mean(mse_train)}\")\n",
    "print(f\"Mean MSE on test data:\\t{np.mean(mse_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45a5c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred, y_test_true = [], []\n",
    "\n",
    "for i in range(y_full.shape[0]):\n",
    "    y_test_pred.append(y_full.iloc[i, :].to_list())\n",
    "    y_test_true.append(test_df.iloc[i, :300].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb6053bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_words, true_words = [], []\n",
    "\n",
    "for i in range(300):\n",
    "    prediction = wv.most_similar(positive=[np.array(y_test_pred[i], dtype='float32')], restrict_vocab=100000, topn=1)[0][0]\n",
    "    pred_words.append(prediction)\n",
    "    true = tfidf_df.iloc[test_df.index[i]][1]\n",
    "    true_words.append(true)\n",
    "\n",
    "tags_comparison = pd.DataFrame(zip(pred_words, true_words), columns=['y_predicted', 'y_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4d55078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NASA</td>\n",
       "      <td>Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>science</td>\n",
       "      <td>Microbes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>cancer</td>\n",
       "      <td>DNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Physics</td>\n",
       "      <td>visible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>biology</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>poetry</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>viruses</td>\n",
       "      <td>biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Death</td>\n",
       "      <td>Transgender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>science</td>\n",
       "      <td>Leadership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>science</td>\n",
       "      <td>Mental_Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>science</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>humanities</td>\n",
       "      <td>antidepressants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>alien</td>\n",
       "      <td>Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>science</td>\n",
       "      <td>Human_Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>biology</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>humanities</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>science</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>apes</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>culture</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_predicted           y_true\n",
       "41        NASA        Astronomy\n",
       "42     science         Microbes\n",
       "43      cancer              DNA\n",
       "44     Physics          visible\n",
       "45     biology          Science\n",
       "46      poetry          culture\n",
       "47     viruses          biology\n",
       "48       Death      Transgender\n",
       "49     science       Leadership\n",
       "50     science    Mental_Health\n",
       "51     science          Science\n",
       "52  humanities  antidepressants\n",
       "53       alien        Astronomy\n",
       "54     science       Human_Body\n",
       "55     biology          Biology\n",
       "56  humanities           London\n",
       "57     science          history\n",
       "58        apes          animals\n",
       "59     culture        education"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_comparison[41:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba157565",
   "metadata": {
    "id": "PgPaIUKPtfLs"
   },
   "source": [
    "Possible further steps: train my own w2v model to make the vector space fit the corpus better; improve data processing and tokenization; predict tags based on several important words according to TF-IDF, not just one; use RNN."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5-better-predictions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
