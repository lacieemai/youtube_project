{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7ba471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idea of this very notebook is to get tags using tf-idf of the terms \n",
    "# we want to count a tf-idf value for each word in each document;\n",
    "# top-5 words with the highest value will become tags (since they are the most valuable for the document)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import utils\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa569b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following downloads may be needed:\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words = list(set().union(nltk_stop_words, sklearn_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b374f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/videos_clean_tags.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0566ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f4061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['``', '--', '\"\"', \"''\", \"?!\", \"...\", \"â€“\"]\n",
    "for p in string.punctuation:\n",
    "    punctuation.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f492046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we tokenize our corpus, remove stopwords, lower the first letter of each sentence\n",
    "# stemming is not used since we want our tags to be actual words, not stems\n",
    "# lemmatization is used so as not to get many similar tags that have the same semantics but different forms\n",
    "# n-grams are also not used here \n",
    "\n",
    "def text_tokenize(text, to_lemmatize=False):\n",
    "    tokenized = []\n",
    "    tokenized_text = nltk.tokenize.sent_tokenize(text)\n",
    "    ignore = list(set().union(stop_words, punctuation))\n",
    "    \n",
    "    for s in tokenized_text:\n",
    "        s = s[0].lower() + s[1:]\n",
    "        tokenized_sent = tokenizer.tokenize(s)\n",
    "        tokenized_sent = [w for w in tokenized_sent if w.lower() not in ignore]\n",
    "        tokenized.append(tokenized_sent)\n",
    "        \n",
    "    tokenized = [item for sublist in tokenized for item in sublist]\n",
    "    \n",
    "    if to_lemmatize:\n",
    "        tag_map = defaultdict(lambda : wn.NOUN)\n",
    "        tag_map['J'] = wn.ADJ\n",
    "        tag_map['V'] = wn.VERB\n",
    "        tag_map['R'] = wn.ADV\n",
    "\n",
    "        lemmatized = []\n",
    "        for token, tag in pos_tag(tokenized):\n",
    "            lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "            lemmatized.append(lemma)\n",
    "        tokenized = lemmatized\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b665a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(data)):\n",
    "    data[i]['captions_tokenized'] = text_tokenize(data[i]['captions'], to_lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41b4d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not the most effective implementation, but it's pretty clear in its steps\n",
    "\n",
    "full_lexicon = {}\n",
    "\n",
    "for document in data:\n",
    "    doc_lexicon = {}\n",
    "    \n",
    "    for term in document['captions_tokenized']:\n",
    "        if doc_lexicon.get(term) is None:\n",
    "            doc_lexicon[term] = 1            \n",
    "        else:\n",
    "            doc_lexicon[term] = doc_lexicon[term] + 1\n",
    "            \n",
    "    norm_term_frequencies = {}\n",
    "    doc_length = len(document)\n",
    "    \n",
    "    for term in doc_lexicon.keys():\n",
    "        norm_term_frequencies[term] = doc_lexicon[term]/doc_length\n",
    "        \n",
    "        if full_lexicon.get(term) is None:\n",
    "            full_lexicon[term] = 1            \n",
    "        else:\n",
    "            full_lexicon[term] = full_lexicon[term] + 1\n",
    "        \n",
    "    document['norm_term_frequencies'] = norm_term_frequencies       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e09cd2f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 0.625),\n",
       " ('trauma', 0.4375),\n",
       " ('story', 0.3125),\n",
       " ('government', 0.25),\n",
       " ('happen', 0.25),\n",
       " ('Adayanci', 0.25),\n",
       " ('girl', 0.25),\n",
       " ('child', 0.25),\n",
       " ('family', 0.1875),\n",
       " ('violence', 0.1875)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what we can achieve with normalized term frequencies only;\n",
    "# actually, the results really help us understand the topic of the text even without the idf part\n",
    "\n",
    "sorted_dict = sorted(data[1]['norm_term_frequencies'].items(), key=lambda x: x[1])\n",
    "sorted_dict.reverse()\n",
    "sorted_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294fbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs = len(data)\n",
    "\n",
    "for document in data:\n",
    "    tf_idf = {}\n",
    "    for term in document['norm_term_frequencies'].keys():\n",
    "        idf = np.log(num_of_docs/full_lexicon[term])\n",
    "        tf_idf[term] = document['norm_term_frequencies'][term] * idf\n",
    "        \n",
    "    tf_idf_sorted = sorted(tf_idf.items(), key=lambda x: x[1])\n",
    "    tf_idf_sorted.reverse()\n",
    "    document['tf_idf_predicted_tags'] = tf_idf_sorted[:5]\n",
    "    document.pop('norm_term_frequencies')\n",
    "    document.pop('captions_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579ebca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('supernova', 6.016755770621052),\n",
       "  ('atom', 3.8321503245419897),\n",
       "  ('star', 3.506738791982303),\n",
       "  ('oxygen', 3.2485690110858716),\n",
       "  ('explosion', 3.2465079603977616)],\n",
       " [{'tag': 'TEDTalk', 'type': 'garbage'},\n",
       "  {'tag': 'TEDTalks', 'type': 'garbage'},\n",
       "  {'tag': 'Astronomy', 'type': 'normal'},\n",
       "  {'tag': 'Science', 'type': 'normal'},\n",
       "  {'tag': 'Universe', 'type': 'normal'},\n",
       "  {'tag': 'Human Origins', 'type': 'normal'},\n",
       "  {'tag': 'Human Body', 'type': 'normal'},\n",
       "  {'tag': 'Cosmos', 'type': 'normal'},\n",
       "  {'tag': 'Humanity', 'type': 'normal'},\n",
       "  {'tag': 'Visualizations', 'type': 'normal'},\n",
       "  {'tag': 'Space', 'type': 'normal'},\n",
       "  {'tag': 'Solar System', 'type': 'normal'}])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# however, now it's even better; though the actual tags are usually different and more abstract\n",
    "\n",
    "n = 427\n",
    "data[n]['tf_idf_predicted_tags'], data[n]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f938f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tfidf_predicted_tags.json', 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "utils.upload_to_googledrive('tfidf_predicted_tags.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
